
# Recurrent Neural Net

- Input
- Weights
- Activation Function
- Output


- Need to store weights
- Choose a type of activation function

---
Artificial Neural Net vs Deep Neural Net
1 hidden layer vs many hidden layers
---

inputs are numeric data points

activation function maps a network's inputs and outputs

non-linear activation functions can:
- allow backpropagation
- allow stacking of layers

types of non-linear activation functions:
- sigmoid/logistic
- tanh/hyperbolic tangent
- relu (rectified linear unit)
- leaky relu
- parametric relu
- softmax
- swish

Neurons are connected to neurons of next layer with weights

